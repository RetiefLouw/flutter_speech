{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Wav2Vec2Model, Wav2Vec2Config\n",
    "from transformers import AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"utter-project/mHuBERT-147\"\n",
    "# model_name_or_path = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "\n",
    "layer = 7\n",
    "output_hidden_states = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. encoder layers 12\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "print(\"No. encoder layers\", config.num_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either enable hidden_states=True or prune layers\n",
    "\n",
    "# Custom config\n",
    "# config.num_hidden_layers = 7\n",
    "if output_hidden_states:\n",
    "   config.output_hidden_states=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_name_or_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-11): 12 x HubertEncoderLayer(\n",
       "    (attention): HubertSdpaAttention(\n",
       "      (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (feed_forward): HubertFeedForward(\n",
       "      (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "      (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features (dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 99, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 2 seconds\n",
    "x_t = torch.randn(1, 32000) # [B x Seq_Len]\n",
    "\n",
    "out = model(x_t)\n",
    "\n",
    "print(out[\"last_hidden_state\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_fn = \"/home/christiaan/Dropbox/code/queries/babaloon_id_11_main_post_test_67591-68366.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12400,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "x, sr = librosa.load(audio_fn, sr=16000)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 38, 768])\n"
     ]
    }
   ],
   "source": [
    "x_t = torch.tensor(x, dtype=torch.float32).view(1,-1)\n",
    "x_t.shape\n",
    "\n",
    "out = model(x_t)\n",
    "print(out[\"last_hidden_state\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features: /home/christiaan/Dropbox/code/adult_templates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:02,  8.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import torch.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# audio_dir = \"/home/christiaan/Dropbox/code/queries\"\n",
    "audio_dir = \"/home/christiaan/Dropbox/code/adult_templates\"\n",
    "\n",
    "\n",
    "# out_dir = f\"out/{model_name_or_path.split('/')[-1]}-layer{layer}/npy\"\n",
    "out_dir = f\"temp_adult\"\n",
    "\n",
    "Path(out_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"Extracting features:\", audio_dir)\n",
    "for wav_fn in tqdm(Path(audio_dir).rglob(\"*.wav\")):\n",
    "\n",
    "    wav, sr = torchaudio.load(wav_fn)\n",
    "    if sr != 16000:\n",
    "       wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    # wav = wav.unsqueeze(0).cuda()\n",
    "    # print(wav.shape)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "    #     wav = F.pad(wav, ((400 - 320) // 2, (400 - 320) // 2))\n",
    "        out = model(wav)\n",
    "    # print(out[\"hidden_states\"][7].shape)\n",
    "    out_fn = f\"{out_dir}/{wav_fn.stem}.npy\"\n",
    "    np.save(out_fn, out[\"hidden_states\"][layer].squeeze().cpu().numpy())\n",
    "    # break\n",
    "\n",
    "    # out_fn = (feat_dir/wav_fn.stem).with_suffix(\".npy\")\n",
    "    # np.save(out_fn, x.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('temp_adult/honger_id_0.npy'), PosixPath('temp_adult/muis_id_0.npy'), PosixPath('temp_adult/hardloop_id_0.npy'), PosixPath('temp_adult/kat_id_0.npy'), PosixPath('temp_adult/katjie_id_0.npy'), PosixPath('temp_adult/vissies_id_0.npy'), PosixPath('temp_adult/seuntjie_id_0.npy'), PosixPath('temp_adult/lekker_id_0.npy'), PosixPath('temp_adult/kwaad_id_0.npy'), PosixPath('temp_adult/butterfly_id_0.npy'), PosixPath('temp_adult/babaloon_id_0.npy'), PosixPath('temp_adult/water_id_0.npy'), PosixPath('temp_adult/sien_id_0.npy'), PosixPath('temp_adult/voel_id_0.npy'), PosixPath('temp_adult/hond_id_0.npy'), PosixPath('temp_adult/gelukkig_id_0.npy'), PosixPath('temp_adult/worsies_id_0.npy'), PosixPath('temp_adult/seer_id_0.npy'), PosixPath('temp_adult/boom_id_0.npy'), PosixPath('temp_adult/balloon_id_0.npy')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 196454.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def speaker_mvn(feat_dict):\n",
    "#     \"\"\"\n",
    "#     Perform per-speaker mean and variance normalisation.\n",
    "\n",
    "#     It is assumed that each of the keys in `feat_dict` starts with a speaker\n",
    "#     identifier followed by an underscore.\n",
    "#     \"\"\"\n",
    "\n",
    "# npy_dir = \"out/wav2vec2-large-xlsr-53-layer12/npy/\"\n",
    "npy_dir = \"temp_adult\"\n",
    "\n",
    "# out_dir = f\"{Path(npy_dir).parent}-mvn/npy\"\n",
    "\n",
    "out_dir = f\"{Path(npy_dir)}-mvn/npy\"\n",
    "\n",
    "Path(out_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "npy_fn_list = [x for x in Path(npy_dir).rglob(\"*.npy\")]\n",
    "print(npy_fn_list)\n",
    "\n",
    "speaker_dict = {}\n",
    "for npy_fn in tqdm(npy_fn_list):\n",
    "    spk = npy_fn.stem.split(\"_\")[2]\n",
    "    if spk not in speaker_dict:\n",
    "        speaker_dict[spk] = []\n",
    "    speaker_dict[spk].append(npy_fn)\n",
    "# print(speaker_dict)\n",
    "\n",
    "for spk in speaker_dict:\n",
    "    print(\"Speaker\", spk)\n",
    "    speaker_features = {}\n",
    "    for npy_fn in speaker_dict[spk]:\n",
    "        speaker_features[npy_fn.stem]= np.load(npy_fn)\n",
    "    # print(len(speaker_features))\n",
    "    # print(speaker_features)\n",
    "\n",
    "    features = np.vstack([speaker_features[x] for x in speaker_features])\n",
    "    speaker_mean = np.mean(features, axis=0)\n",
    "    speaker_std = np.std(features, axis=0)\n",
    "\n",
    "    out_dict = {}\n",
    "    for utt_key in speaker_features:\n",
    "        out_dict[utt_key] = (\n",
    "            (speaker_features[utt_key] - speaker_mean) / \n",
    "            speaker_std\n",
    "        )\n",
    "    # print(out_dict)\n",
    "    for utt_key in out_dict:\n",
    "        np.save(f\"{out_dir}/{utt_key}.npy\", out_dict[utt_key])\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
